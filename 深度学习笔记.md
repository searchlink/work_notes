```
Momentum解决SGD的两个问题：
1. 收敛速度
2. 局部最优
```

```
为何需要激活函数？
只能学习线性变换，即假设空间受限。多层堆叠的结果还是只能学习线性变换。
```

```
关于规范化或者词汇约束方面：
应该使用训练数据，而不是测试数据。不要使用任何基于测试数据的指标
```
```
关于缺失值：
如果缺失值存在于测试集，而不存在于训练集，神经网络将不会学习到任何关于缺失值的信息，需要人工在训练样本中生成部分缺失值。
```

```
特征工程：
在于不仅将数据处理成神经网络方便处理的格式，同时很重要的是减小计算量和抽象出问题的本质
```

```
小网络相比大网络而言， 过拟合要晚，并且一旦过拟合之后性能衰减的更慢。(拟合较慢)
```

```
为了减小过拟合，有两种手段：
1. 减小layer的参数个数
2. 增加权重正则化(weight regularization)
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))
3. dropout方法
训练阶段使用，测试阶段不使用。
4. 增加训练数据
```

```
关于dropout：
为什么dropout需要缩放？
因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。
```

```
metrics的选择：
对于平衡的分类问题： accuracy和AUC是commen choice。
对于不平衡的分类问题： precision和recall是commen choice。
```

```
为何使用downsampling？因为降采样不仅可以缩小参数个数，而且可以学习空间层次结构。
通过pooling和stride方式。
```

```
关于图像的预处理：
1. 读取picture files
2. 解码jpeg内容到rgb像素网格
3. 转变为floating-point tensor
4. rescale重新调整像素值

from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir,target_size=(150, 150),batch_size=20,class_mode='binary')
validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=20,class_mode='binary')
```

```
生成器：
需要指明steps_per_epoch和validation_steps，因为迭代永不止境
history = model.fit_generator(
train_generator,
steps_per_epoch=100,
epochs=30,
validation_data=validation_generator,
validation_steps=50)
```

```
训练之后保存模型：
model.save('cats_and_dogs_small_1.h5')
```

```
利用经典的模型架构来进行特征抽取，不需要训练权重
关于特征抽取有两种方式：
1. 在数据上运行vgg conv base, 直接predict(), 抽取特征。然后单独接一个全连接层进行分类训练。但是不允许做数据增强。
2. 扩展vgg conv base，通过添加dense layer来打通模型，允许数据增强。
此时需要freeze layer
conv_base.trainable = False(避免训练过程中更新权重参数)
```

```
fine-tuning(微调):
基于经典的模型架构进行模型架构的微调，此时需要训练部分层的参数权重。
1. Add your custom network on top of an already-trained base network.
在base network上添加自己的自定义网络
2. Freeze the base network.
冻结base network
3. Train the part you added.
训练添加的自定义网络
4. Unfreeze some layers in the base network.
解冻base network的某些layers
5. Jointly train both these layers and the part you added.
联合训练自定义添加的layer和解冻的layers

fine-tuning示例：
conv_base.trainable = True
set_trainable = False
for layer in conv_base.layers:
if layer.name == 'block5_conv1':
    set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False
此时微调需要设置非常小的learning rate.避免fine-tuning的几层layers大量的变动。
model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-5), metrics=['acc'])
```

    